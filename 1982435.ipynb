{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunpathy/fifa_wage_predictor/blob/main/1982435.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Global Variables"
      ],
      "metadata": {
        "id": "w1WCncFCOKmc"
      },
      "id": "w1WCncFCOKmc"
    },
    {
      "cell_type": "code",
      "source": [
        "GDRIVE_FILE_URL =  'gdrive/My Drive/players_22.csv'\n",
        "FILE_NAME = \"CleanedDataSet.csv\"\n",
        "FILTER_PLAYER_POSITION = \"DEFENDER\" # \"DEFENDER\" \"ATTACKER\""
      ],
      "metadata": {
        "id": "AmNGkWbSOJCz"
      },
      "id": "AmNGkWbSOJCz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements\n"
      ],
      "metadata": {
        "id": "VbWRm_WlI1J0"
      },
      "id": "VbWRm_WlI1J0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "181c89ce",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "181c89ce",
        "outputId": "e0cf3921-49e6-481e-aaf5-9ee9de9937ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 33 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 44.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=e0c839838ccbf0c2c9a5c133311c18980a317216563569a9adf77d65c9eeab4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.feature import VectorIndexer, VectorAssembler, MinMaxScaler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.mllib.util import MLUtils\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc = SparkContext('local')\n",
        "spark = SparkSession(sc)"
      ],
      "metadata": {
        "id": "uGEWehnl76Gk"
      },
      "id": "uGEWehnl76Gk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding k best Input Features using SelectKBest() library"
      ],
      "metadata": {
        "id": "XshGwLJrCmzA"
      },
      "id": "XshGwLJrCmzA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cc0dbab",
      "metadata": {
        "id": "0cc0dbab"
      },
      "outputs": [],
      "source": [
        "def kBestInputFeatures(k, position, data):\n",
        "    from sklearn.feature_selection import SelectKBest, f_regression\n",
        "\n",
        "    data = data.filter(data.position == FILTER_PLAYER_POSITION).toPandas()\n",
        "\n",
        "\n",
        "    cols =[\"age\",\"overall\" ,\"potential\",\"pace\",\"shooting\",\"passing\",\"dribbling\",\"defending\",\"physic\",\n",
        "           \"attacking_crossing\",\"attacking_finishing\",\"attacking_heading_accuracy\",\"attacking_short_passing\",\n",
        "           \"attacking_volleys\",\"skill_dribbling\",\"skill_curve\",\"skill_fk_accuracy\",\"skill_long_passing\",\n",
        "           \"skill_ball_control\",\"movement_acceleration\",\"movement_sprint_speed\",\"movement_agility\",\n",
        "           \"movement_reactions\",\"movement_balance\",\"power_shot_power\",\"power_jumping\",\"power_stamina\",\n",
        "           \"power_strength\",\"power_long_shots\",\"mentality_aggression\",\"mentality_interceptions\",\n",
        "           \"mentality_positioning\",\"mentality_vision\",\"mentality_penalties\",\"mentality_composure\",\n",
        "           \"defending_marking_awareness\",\"defending_sliding_tackle\",\n",
        "           \"goalkeeping_diving\",\"goalkeeping_handling\",\"goalkeeping_kicking\",\"goalkeeping_positioning\",\n",
        "           \"goalkeeping_reflexes\"]\n",
        "    \n",
        "    X1 = data[cols]\n",
        "    y1 = data[['wage_eur']]\n",
        "    \n",
        "    select = SelectKBest(score_func=f_regression, k = k )\n",
        "    z = select.fit_transform(X1, y1) \n",
        "\n",
        "    print(\"After selecting best \"+ str(k) +\" features:\", z.shape) \n",
        "\n",
        "    filter = select.get_support()\n",
        "    features = np.array(X1.columns)\n",
        "\n",
        "\n",
        "    # print(\"All features:\")\n",
        "    # print(features)\n",
        "\n",
        "    print(\"\\nSelected best \" ,k,'features for ',position)\n",
        "    print(features[filter])\n",
        "    # print(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleanup"
      ],
      "metadata": {
        "id": "DCKWvv5eCwBg"
      },
      "id": "DCKWvv5eCwBg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7dbeedf",
      "metadata": {
        "id": "d7dbeedf"
      },
      "outputs": [],
      "source": [
        "def iqr_capping(df,factor,spark):\n",
        "    q1= df['wage_eur'].quantile(0.25)\n",
        "    q3 = df['wage_eur'].quantile(0.75)\n",
        "    iqr = q3-q1\n",
        "    upper_whisker=q3+(factor*iqr)\n",
        "    lower_whisker = q1-(factor*iqr)\n",
        "    df['wage_eur'] = np.where(df['wage_eur']>upper_whisker,upper_whisker,\n",
        "                 np.where(df['wage_eur']<lower_whisker,lower_whisker,df['wage_eur']))\n",
        "    df.boxplot(['wage_eur'])\n",
        "    return spark.createDataFrame(df) \n",
        "\n",
        "def dataCleanupAndFromat(filename):    \n",
        "    cols =['sofifa_id','short_name','club_name','nationality_name','age',\"overall\" ,\"potential\",\"pace\",\"shooting\",\"passing\",\"dribbling\",\"defending\",\"physic\",\n",
        "    \"attacking_crossing\",\"attacking_finishing\",\"attacking_heading_accuracy\",\"attacking_short_passing\",\n",
        "    \"attacking_volleys\",\"skill_dribbling\",\"skill_curve\",\"skill_fk_accuracy\",\"skill_long_passing\",\n",
        "    \"skill_ball_control\",\"movement_acceleration\",\"movement_sprint_speed\",\"movement_agility\",\n",
        "    \"movement_reactions\",\"movement_balance\",\"power_shot_power\",\"power_jumping\",\"power_stamina\",\n",
        "    \"power_strength\",\"power_long_shots\",\"mentality_aggression\",\"mentality_interceptions\",\n",
        "    \"mentality_positioning\",\"mentality_vision\",\"mentality_penalties\",\"mentality_composure\",\n",
        "    \"defending_marking_awareness\",\"defending_standing_tackle\",\"defending_sliding_tackle\",\n",
        "    \"goalkeeping_diving\",\"goalkeeping_handling\",\"goalkeeping_kicking\",\"goalkeeping_positioning\",\n",
        "    \"goalkeeping_reflexes\",'wage_eur']\n",
        "    \n",
        "    df  = pd.read_csv(filename)\n",
        "\n",
        "    defense = [\"LWB\",\"RWB\",\"CDM\",\"LB\",\"RB\",\"CB\"]\n",
        "    attack = ['CF',\"CM\",'ST',\"CAM\",'LW','RM','RW','LM']\n",
        "    positions = []\n",
        "    \n",
        "    for pos in df[\"player_positions\"]:\n",
        "        pos = pos.replace(\" \", \"\").split(\",\")\n",
        "        pos = \"ATTACKER\" if pos[0] in attack else \"DEFENDER\" if pos[0] in defense else \"GOAL KEEPER\"\n",
        "        positions.append(pos)\n",
        "\n",
        "    df['position'] = positions\n",
        "    cols.append(\"position\")\n",
        "    df = df[cols]\n",
        "    df = df.replace([np.inf, -np.inf], np.nan).dropna(axis=0)\n",
        "\n",
        "    # To write a csv file\n",
        "    df.to_csv(FILE_NAME, sep=',', encoding='utf-8')\n",
        "    print(\"Formatted Data Written to the file : \" , FILE_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning for Random Forest"
      ],
      "metadata": {
        "id": "z8uFQwNn6Kiz"
      },
      "id": "z8uFQwNn6Kiz"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def bestRFParameters(train_features, train_label, estimators, depth):\n",
        "    from sklearn.ensemble import RandomForestRegressor\n",
        "    rfc = RandomForestRegressor()\n",
        "    parameters = {\n",
        "        \"n_estimators\": estimators,\n",
        "        \"max_depth\": depth\n",
        "    }\n",
        "    \n",
        "    from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "    cv = GridSearchCV(rfc,parameters,cv = 5)\n",
        "    cv.fit(train_features,train_label.values.ravel())\n",
        "\n",
        "    def display(results):\n",
        "        print(f'Best parameters are: {results.best_params_}')\n",
        "        print(\"\\n\")\n",
        "        mean_score = results.cv_results_['mean_test_score']\n",
        "        std_score = results.cv_results_['std_test_score']\n",
        "        params = results.cv_results_['params']\n",
        "        for mean,std,params in zip(mean_score,std_score,params):\n",
        "            print(f'{round(mean,3)} + or -{round(std,3)} for the {params}')\n",
        "\n",
        "    display(cv)\n",
        "\n",
        "data = spark.read.csv(FILE_NAME,inferSchema=True, header=True)\n",
        "data = iqr_capping(data.toPandas(),1.5,spark)\n",
        "\n",
        "if FILTER_PLAYER_POSITION == \"DEFENDER\" :\n",
        "    cols = ['overall', 'defending', 'movement_reactions', 'mentality_interceptions','defending_sliding_tackle','wage_eur'] #DEFENDER\n",
        "else :   \n",
        "    cols = ['overall' ,'dribbling', 'skill_dribbling' , 'skill_ball_control','movement_reactions','wage_eur'] #ATTACKER\n",
        "\n",
        "data = data.filter(data.position == FILTER_PLAYER_POSITION)\n",
        "data = data.select(cols)\n",
        "\n",
        "(train, test) = data.randomSplit([0.8, 0.2])\n",
        "assembler=VectorAssembler().setInputCols(cols).setOutputCol('features')\n",
        "train_a = assembler.transform(train)\n",
        "train_b = train_a.select(\"features\",train_a.wage_eur.alias('label'))\n",
        "\n",
        "\n",
        "X_train = train_a.select(cols).toPandas()\n",
        "y_train = train_b.select(\"label\").toPandas()\n",
        "\n",
        "\n",
        "\n",
        "bestRFParameters(X_train, y_train, [21,22,23,24,25],[2, 4, 6, 8]) # Takes some time...\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "dvAp0AJ56InT",
        "outputId": "08d17639-eb00-4e91-b740-085f2c759721"
      },
      "id": "dvAp0AJ56InT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b8b88adfbd5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILE_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miqr_capping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/content/CleanedDataSet.csv"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display Results"
      ],
      "metadata": {
        "id": "D2i0yFJnCtMZ"
      },
      "id": "D2i0yFJnCtMZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96a137d4",
      "metadata": {
        "id": "96a137d4"
      },
      "outputs": [],
      "source": [
        "def getResult (model,lib) : \n",
        "    evaluator = RegressionEvaluator()\n",
        "    r2 = (evaluator.evaluate(model,{evaluator.metricName: \"r2\"}))\n",
        "    mae =(evaluator.evaluate(model,{evaluator.metricName: \"mae\"}))\n",
        "    mse = (evaluator.evaluate(model,{evaluator.metricName: \"mse\"}))\n",
        "    rmse = (evaluator.evaluate(model,{evaluator.metricName: \"rmse\"}))\n",
        "    return {\"lib\":lib,\"r2\":r2,\"mae\":mae,\"mse\":mse,\"rmse\":rmse}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AKH3j0ZEOHD7"
      },
      "id": "AKH3j0ZEOHD7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Data"
      ],
      "metadata": {
        "id": "LhNbkES_Ch_G"
      },
      "id": "LhNbkES_Ch_G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b2e5b4e",
      "metadata": {
        "scrolled": false,
        "id": "4b2e5b4e"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "dataCleanupAndFromat(GDRIVE_FILE_URL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af0f6d43",
      "metadata": {
        "scrolled": true,
        "id": "af0f6d43"
      },
      "outputs": [],
      "source": [
        "data = spark.read.csv(FILE_NAME,inferSchema=True, header=True)\n",
        "data = iqr_capping(data.toPandas(),1.5,spark)\n",
        "\n",
        "useful_columns =['sofifa_id','short_name','club_name','age','nationality_name','overall','potential' ,'dribbling', 'attacking_crossing','skill_dribbling', 'skill_ball_control',\n",
        " 'movement_reactions' ,'defending', 'defending_standing_tackle', 'defending_sliding_tackle','mentality_interceptions','position','wage_eur']\n",
        "mapData = data.select(useful_columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Heatmap Corrrelation for some of Input Features"
      ],
      "metadata": {
        "id": "i38gdE4ECEtQ"
      },
      "id": "i38gdE4ECEtQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62a6026a",
      "metadata": {
        "id": "62a6026a"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "plt.figure(figsize=(15,8))\n",
        "cor = mapData.toPandas().corr()\n",
        "sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Handling\n"
      ],
      "metadata": {
        "id": "UX13iagnCJYn"
      },
      "id": "UX13iagnCJYn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aaad521d",
      "metadata": {
        "id": "aaad521d"
      },
      "outputs": [],
      "source": [
        "\n",
        "kBestInputFeatures(5, FILTER_PLAYER_POSITION, data )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbf73374",
      "metadata": {
        "id": "dbf73374"
      },
      "outputs": [],
      "source": [
        "data = data.filter(data.position == FILTER_PLAYER_POSITION)\n",
        "\n",
        "if FILTER_PLAYER_POSITION == \"DEFENDER\" :\n",
        "    cols = ['overall', 'defending', 'movement_reactions', 'mentality_interceptions','defending_sliding_tackle','wage_eur'] #DEFENDER\n",
        "else :   \n",
        "    cols = ['overall' ,'dribbling', 'skill_dribbling' , 'skill_ball_control','movement_reactions','wage_eur'] #ATTACKER\n",
        "\n",
        "data2 = data.select(cols)\n",
        "print((data.count(), len(data.columns)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe71bb7",
      "metadata": {
        "scrolled": true,
        "id": "9fe71bb7"
      },
      "outputs": [],
      "source": [
        "(train, test) = data2.randomSplit([0.8, 0.2])\n",
        "cols.remove('wage_eur')\n",
        "assembler=VectorAssembler().setInputCols(cols).setOutputCol('features')\n",
        "train_a = assembler.transform(train)\n",
        "train_b = train_a.select(\"features\",train_a.wage_eur.alias('label'))\n",
        "# train_b.show(truncate=False)\n",
        "test_a =  assembler.transform(test)\n",
        "test_b = test_a.select('features', test_a.wage_eur.alias('label'))\n",
        "\n",
        "results = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Regression"
      ],
      "metadata": {
        "id": "YnVQaQQvBxOP"
      },
      "id": "YnVQaQQvBxOP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7060d475",
      "metadata": {
        "scrolled": false,
        "id": "7060d475"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "dt = DecisionTreeRegressor(maxDepth=5)\n",
        "\n",
        "model = dt.fit(train_b)\n",
        "test_dt = model.transform(test_b)\n",
        "# test_dt.show(truncate=False)\n",
        "\n",
        "results.append(getResult(test_dt,'dt'))\n",
        "\n",
        "list1= test_dt.select('label')\n",
        "list2 = test_dt.select('prediction')\n",
        "wage = [int(row.label) for row in list1.collect()]\n",
        "prediction = [int(row.prediction) for row in list2.collect()]\n",
        "\n",
        "plt.plot(wage)\n",
        "plt.plot(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Regression"
      ],
      "metadata": {
        "id": "SJv-EhjCB4tC"
      },
      "id": "SJv-EhjCB4tC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13227f73",
      "metadata": {
        "scrolled": true,
        "id": "13227f73"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "rf = RandomForestRegressor(numTrees = 20, maxDepth= 5)\n",
        "model = rf.fit(train_b)\n",
        "test_rf = model.transform(test_b)\n",
        "# test_rf.show(truncate=False)\n",
        "\n",
        "results.append(getResult(test_rf,'rf'))\n",
        "\n",
        "list1= test_rf.select('label')\n",
        "list2 = test_rf.select('prediction')\n",
        "wage = [int(row.label) for row in list1.collect()]\n",
        "prediction = [int(row.prediction) for row in list2.collect()]\n",
        "\n",
        "plt.plot(wage)\n",
        "plt.plot(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Isotonic Regression"
      ],
      "metadata": {
        "id": "5-1JERgbB9bp"
      },
      "id": "5-1JERgbB9bp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30378278",
      "metadata": {
        "id": "30378278"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pyspark.ml.regression import IsotonicRegression\n",
        "ir = IsotonicRegression()\n",
        "test_a =  assembler.transform(test)\n",
        "test_b = test_a.select('features', test_a.wage_eur.alias('label'))\n",
        "model = ir.fit(train_b)\n",
        "test_ir = model.transform(test_b)\n",
        "# test_ir.show(truncate=False)\n",
        "results.append(getResult(test_ir,'ir'))\n",
        "\n",
        "\n",
        "list1= test_ir.select('label')\n",
        "list2 = test_ir.select('prediction')\n",
        "wage = [int(row.label) for row in list1.collect()]\n",
        "prediction = [int(row.prediction) for row in list2.collect()]\n",
        "\n",
        "plt.plot(wage)\n",
        "plt.plot(prediction)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN Regression"
      ],
      "metadata": {
        "id": "hUgUOH_rHowL"
      },
      "id": "hUgUOH_rHowL"
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN by Pandas\n",
        "def knn_regression(train_a,train_b,test_a,test_b, cols):\n",
        "\n",
        "\n",
        "  from sklearn.neighbors import KNeighborsRegressor\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn import metrics\n",
        "  import numpy as np\n",
        "  from matplotlib import pyplot as plt \n",
        "\n",
        "\n",
        "  X_train = train_a.select(cols).toPandas()\n",
        "  y_train = train_b.select(\"label\").toPandas()\n",
        "  X_test = test_a.select(cols).toPandas()\n",
        "  y_test = test_b.select(\"label\").toPandas()\n",
        "\n",
        "  knn_model = KNeighborsRegressor(n_neighbors = 10 , algorithm = 'brute')\n",
        "  knn_model.fit(X_train, y_train)\n",
        "  knn_pred_wage = knn_model.predict(X_test)\n",
        "\n",
        "  r2 = metrics.r2_score(y_test, knn_pred_wage)\n",
        "  mse = metrics.mean_squared_error(y_test, knn_pred_wage)\n",
        "  mae = metrics.mean_absolute_error(y_test, knn_pred_wage)\n",
        "  rmse = np.sqrt(mse)\n",
        "\n",
        "  #Plotting Graph\n",
        "  y_test = y_test['label'].values\n",
        "  knn_pred_wage = knn_pred_wage.reshape(-1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  plt.plot(y_test)\n",
        "  plt.plot(knn_pred_wage)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  return {\"lib\":'knn',\"r2\":float(r2),\"mae\":float(mae),\"mse\":float(mse),\"rmse\":float(rmse)}\n",
        "\n",
        "results.append(knn_regression(train_a,train_b,test_a,test_b, cols))"
      ],
      "metadata": {
        "id": "h2RFUHrpDFaI"
      },
      "id": "h2RFUHrpDFaI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ],
      "metadata": {
        "id": "mlqQgRGv5x1k"
      },
      "id": "mlqQgRGv5x1k"
    },
    {
      "cell_type": "code",
      "source": [
        "#SVM by Pandas\n",
        "def smv_regression(train_a,train_b,test_a,test_b, cols):\n",
        "  from sklearn.svm import SVR\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn import metrics\n",
        "  import numpy as np\n",
        "  from matplotlib import pyplot as plt \n",
        "\n",
        "  \n",
        "  X_train = train_a.select(cols).toPandas()\n",
        "  y_train = train_b.select(\"label\").toPandas()\n",
        "  X_test = test_a.select(cols).toPandas()\n",
        "  y_test = test_b.select(\"label\").toPandas()\n",
        "\n",
        "  svm_model = SVR(kernel = 'rbf')\n",
        "\n",
        "  svm_model.fit(X_train, y_train)\n",
        "  svm_pred_wage = svm_model.predict(X_test)\n",
        "\n",
        "  r2 = metrics.r2_score(y_test, svm_pred_wage)\n",
        "  mse = metrics.mean_squared_error(y_test, svm_pred_wage)\n",
        "  mae = metrics.mean_absolute_error(y_test, svm_pred_wage)\n",
        "  rmse = np.sqrt(mse)\n",
        "\n",
        "  #Plotting Graph\n",
        "  y_test = y_test['label'].values\n",
        "  svm_pred_wage = svm_pred_wage.reshape(-1)\n",
        "\n",
        "  plt.plot(y_test)\n",
        "  plt.plot(svm_pred_wage)\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  return {\"lib\":'svm',\"r2\":float(r2),\"mae\":float(mae),\"mse\":float(mse),\"rmse\":float(rmse)}\n",
        "\n",
        "results.append(smv_regression(train_a,train_b,test_a,test_b, cols))"
      ],
      "metadata": {
        "id": "83ZXs87n5w-f"
      },
      "id": "83ZXs87n5w-f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OVERALL RESULTS"
      ],
      "metadata": {
        "id": "iPBPimhVBswL"
      },
      "id": "iPBPimhVBswL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65645d20",
      "metadata": {
        "id": "65645d20"
      },
      "outputs": [],
      "source": [
        "\n",
        "results_df = spark.createDataFrame(results)\n",
        "print(\"\\n\\n Data set Year : \", \"2022\",\"Position : \", FILTER_PLAYER_POSITION)\n",
        "results_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Club Wise Best Overall Performer's Wage Comparison"
      ],
      "metadata": {
        "id": "sHQniJqpBjpz"
      },
      "id": "sHQniJqpBjpz"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ys5xAFmeBjm_"
      },
      "id": "ys5xAFmeBjm_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9abc4341",
      "metadata": {
        "id": "9abc4341"
      },
      "outputs": [],
      "source": [
        "def clubWisePerformance():\n",
        "    import plotly.express as px\n",
        "    club_names = ['Real Madrid CF','FC Barcelona','Manchester United','Juventus','Chelsea', \n",
        "                  'Paris Saint-Germain' ,'FC Bayern München', 'Arsenal', 'Liverpool', 'Manchester City']\n",
        "\n",
        "    data_frame = spark.read.csv(FILE_NAME,inferSchema=True, header=True).toPandas()\n",
        "    data_frame  = data_frame[data_frame.club_name.isin(club_names)]\n",
        "    data_frame = data_frame[['short_name','club_name','overall','wage_eur']]\n",
        "    data_frame = data_frame.sort_values(by=['overall','club_name'], ascending=False)\n",
        "    data_frame= data_frame.reset_index()\n",
        "\n",
        "    names = []\n",
        "    club = []\n",
        "    scores = []\n",
        "    max_scores = data_frame.groupby(['club_name'], sort=False)['overall'].max()\n",
        "    for ind in data_frame.index:\n",
        "     if max_scores[data_frame['club_name'][ind]] == data_frame['overall'][ind] :\n",
        "         names.append(data_frame['short_name'][ind])\n",
        "         club. append(data_frame['club_name'][ind])\n",
        "         scores.append(data_frame['wage_eur'][ind])\n",
        "    \n",
        "    fig = px.bar(scores, x=names, y=scores,color= club, title=\"Club Wise Best Overall Performer's Wage Comparison\")\n",
        "    fig.show()\n",
        "\n",
        "    \n",
        "clubWisePerformance()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting Top K players within Given Budget"
      ],
      "metadata": {
        "id": "lIUXXymmBeSh"
      },
      "id": "lIUXXymmBeSh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "289204ad",
      "metadata": {
        "id": "289204ad"
      },
      "outputs": [],
      "source": [
        "def selectTopKPlayersWithinBudget(k,pos,nation,max_budget,spark):\n",
        "    import plotly.express as px\n",
        "    useful_columns =['sofifa_id','short_name','overall','age','wage_eur','potential','club_name','nationality_name']\n",
        "    \n",
        "    df = spark.read.csv(FILE_NAME,inferSchema=True, header=True).toPandas()\n",
        "    if nation != \"all\":\n",
        "        df = df[df['nationality_name'] == nation]\n",
        "    query = \"position =='\"+pos+\"' & wage_eur <= \"+str(max_budget)\n",
        "    topKPlayers =  df.query(query)[useful_columns]\n",
        "    topKPlayers = topKPlayers.sort_values('overall', ascending=False).iloc[0:k]\n",
        "    fig = px.bar(topKPlayers, x=\"short_name\", y=\"wage_eur\", color='nationality_name', title=\"TOP \"+str(k)+\" \"+pos+'S Under '+str(max_budget)+\" Euros\")\n",
        "    fig.show()      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "country = \"all\" # all , Italy , Germany , Portugal , England , India ...\n",
        "max_budget = 360000\n",
        "\n",
        "selectTopKPlayersWithinBudget(10,\"ATTACKER\",country,max_budget,spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing given 2 players"
      ],
      "metadata": {
        "id": "jdShHCujBU7N"
      },
      "id": "jdShHCujBU7N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6febeb3",
      "metadata": {
        "id": "c6febeb3"
      },
      "outputs": [],
      "source": [
        "def comparePlayers(id1,id2):\n",
        "    import plotly.graph_objects as go\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "    df = spark.read.csv(FILE_NAME,inferSchema=True, header=True).toPandas()\n",
        "\n",
        "    scaler = MinMaxScaler(feature_range=(1 , 100))\n",
        "    features = df[[\"wage_eur\"]]\n",
        "    df[[\"wage_eur\"]] = scaler.fit_transform(features.values)\n",
        "    \n",
        "    player1 = df[df[\"sofifa_id\"]== int(id1)]\n",
        "    player2 = df[df[\"sofifa_id\"]== int(id2)]\n",
        "   \n",
        "    categories = [\"overall\",\"potential\", \"pace\", \"shooting\" ,\"dribbling\" ,\"defending\" ,\"physic\",'passing','wage_eur']\n",
        "    p1 = player1[categories].to_numpy()[0]\n",
        "    p2 = player2[categories].to_numpy()[0]\n",
        "    \n",
        "    fig = go.Figure()\n",
        "\n",
        "    fig.add_trace(go.Scatterpolar(r= p1,theta=categories,fill='toself',\n",
        "                                  name = player1['short_name'].values[0]))\n",
        "    \n",
        "    fig.add_trace(go.Scatterpolar(r= p2,theta=categories,fill='toself',\n",
        "                                  name = player2['short_name'].values[0]))\n",
        "\n",
        "    fig.update_layout(polar=dict(radialaxis=dict(visible=True,range=[1, 100])),showlegend = True,title='Player Comparison')\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "comparePlayers(192985, 158023 ) #ids: 20801 , 158023 , 192985 , 261962 , 231747"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "ap8OTPJJdyfY"
      },
      "id": "ap8OTPJJdyfY",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DCKWvv5eCwBg",
        "D2i0yFJnCtMZ",
        "LhNbkES_Ch_G",
        "i38gdE4ECEtQ",
        "UX13iagnCJYn",
        "YnVQaQQvBxOP"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}